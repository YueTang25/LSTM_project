# LSTM_project
Final project of Neural Networks and Deep Learning (CSCI_6366)

## Project Description
To manage natural language sentences and time series data, Hochreiter and Schmidhuber proposed the Recurrent Neural Network (RNN), a highly popular model in deep learning. However, RNNs often underperform with longer sentences, prompting the same authors to introduce the Long Short-Term Memory (LSTM) model. In the LSTM, they incorporated a gated mechanism to maintain both long-term and short-term memories effectively.

In this project, I implement the LSTM model and provide a detailed discussion of its framework. Additionally, I explore the distinctions between RNNs and LSTMs, highlighting the improvements LSTMs offer in handling longer data sequences.

Recurrent Neural Networks (RNNs): A gentle Introduction and Overview: https://arxiv.org/abs/1912.05911
Long Short-Term Memory: https://ieeexplore.ieee.org/abstract/document/6795963
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling: https://arxiv.org/pdf/1412.3555

## Team Member
It's a individual project, the author is: Yue Tang

## Dataset
To demonstrate the performance of vanilla RNN and LSTM, I used consin value generated by python function to show the periodical time series. I also used Acohol Sale Dataset as uploaded.
